{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQxC1wVeFDUp"
      },
      "source": [
        "# Colab Inference for MedleyVox\n",
        "\n",
        "Medley Vox is a [dataset for testing algorithms for separating multiple singers](https://arxiv.org/pdf/2211.07302) within a single music track. Also, the [authors of Medley Vox](https://github.com/jeonchangbin49/MedleyVox) proposed a neural network architecture for separating singers. However, unfortunately, they did not publish the weights. Later, their training process was [repeated by Cyru5](https://huggingface.co/Cyru5/MedleyVox/tree/main), who trained several models and published the weights in the public domain. Now this WebUI is created to use the trained models and weights for inference. Here are some precautions:\n",
        "1. Put the [downloaded models](https://huggingface.co/Cyru5/MedleyVox) in the 'checkpoints' folder in folder format, with each model folder containing a model file (.pth) and its corresponding configuration file (.json).\n",
        "2. If you use overlapadd and the choice of model is 'w2v' or 'w2v_chunk', you need to download the pretrained model [xlsr_53_56k.pt](https://dl.fbaipublicfiles.com/fairseq/wav2vec/xlsr_53_56k.pt) and put it in the 'pretrained' folder.\n",
        "3. At present, the audio output sampling rate supported by the model is 24000kHz and cannot be changed. To solve this, you can use [AudioSR](https://github.com/haoheliu/versatile_audio_super_resolution), [Apollo](https://github.com/JusperLee/Apollo), or [Music Source Separation Training](https://github.com/ZFTurbo/Music-Source-Separation-Training) for audio super-resolution.\n",
        "4. When using WebUI on cloud platforms or Colab, please place the audio to be processed in the 'inputs' folder, and the processing results will be stored in the 'results' folder. The 'Select folder' and 'Open folder' buttons are invalid in the cloud.\n",
        "5. If the input is too long, it may be impossible to inference due to lack of VRAM. In that case, use 'use_overlapadd'. Among the 'use_overlapadd' options, \"ola\", \"ola_norm\", and \"w2v\" all work well. Use w2v_chunk or sf_chunk if these fail or as desired. You can also try experimenting with 'vad_method' options spec and webrtc when using either of the \"_chunk\" methods. Chunking has proven to be very useful therefore it is on by default."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2tqg-ysFDUw"
      },
      "source": [
        "# Initialize environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpM1R3aKFDUx",
        "outputId": "7777a2ee-d343-4938-fa2c-1598d5fa50d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue May  6 03:46:14 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "Cloning into 'MedleyVox-Inference-WebUI'...\n",
            "remote: Enumerating objects: 71, done.\u001b[K\n",
            "remote: Counting objects: 100% (71/71), done.\u001b[K\n",
            "remote: Compressing objects: 100% (51/51), done.\u001b[K\n",
            "remote: Total 71 (delta 26), reused 61 (delta 18), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (71/71), 49.60 KiB | 806.00 KiB/s, done.\n",
            "Resolving deltas: 100% (26/26), done.\n",
            "/content/MedleyVox-Inference-WebUI\n",
            "Requirement already satisfied: pip==24.0 in /usr/local/lib/python3.11/dist-packages (24.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (80.3.1)\n",
            "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting git+https://github.com/liyaodev/fairseq.git\n",
            "  Cloning https://github.com/liyaodev/fairseq.git to /tmp/pip-req-build-6ljmmig_\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/liyaodev/fairseq.git /tmp/pip-req-build-6ljmmig_\n",
            "  Resolved https://github.com/liyaodev/fairseq.git to commit b963eac7a04c539ad59fb1e23277f2ff7ee29e74\n",
            "  Running command git submodule update --init --recursive -q\n"
          ]
        }
      ],
      "source": [
        "#@title Clone repository and install requirements\n",
        "#@markdown # Clone repository and install requirements\n",
        "#@markdown\n",
        "\n",
        "!nvidia-smi\n",
        "!git clone https://github.com/SUC-DriverOld/MedleyVox-Inference-WebUI\n",
        "%cd /content/MedleyVox-Inference-WebUI\n",
        "!python -m pip install --upgrade pip==24.0 setuptools\n",
        "!python -m pip install git+https://github.com/liyaodev/fairseq.git\n",
        "!python -m pip install -r requirements.txt --extra-index-url https://download.pytorch.org/whl/cu124\n",
        "!mkdir -p inputs\n",
        "!mkdir -p results\n",
        "\n",
        "!mkdir -p \"/content/MedleyVox-Inference-WebUI/checkpoint/vocals_238\"\n",
        "%cd \"/content/MedleyVox-Inference-WebUI/checkpoint/vocals_238\"\n",
        "!wget https://huggingface.co/Cyru5/MedleyVox/resolve/main/vocals%20238/vocals.pth\n",
        "!wget https://huggingface.co/Cyru5/MedleyVox/resolve/main/vocals%20238/vocals.json\n",
        "!mkdir -p \"/content/MedleyVox-Inference-WebUI/checkpoint/multi_singing_librispeech_138\"\n",
        "%cd \"/content/MedleyVox-Inference-WebUI/checkpoint/multi_singing_librispeech_138\"\n",
        "!wget https://huggingface.co/Cyru5/MedleyVox/resolve/main/multi_singing_librispeech_138/vocals.pth\n",
        "!wget https://huggingface.co/Cyru5/MedleyVox/resolve/main/multi_singing_librispeech_138/vocals.json\n",
        "!mkdir -p \"/content/MedleyVox-Inference-WebUI/checkpoint/singing_librispeech_ft_iSRNet\"\n",
        "%cd \"/content/MedleyVox-Inference-WebUI/checkpoint/singing_librispeech_ft_iSRNet\"\n",
        "!wget https://huggingface.co/Cyru5/MedleyVox/resolve/main/singing_librispeech_ft_iSRNet/vocals.pth\n",
        "!wget https://huggingface.co/Cyru5/MedleyVox/resolve/main/singing_librispeech_ft_iSRNet/vocals.json\n",
        "!mkdir -p \"/content/MedleyVox-Inference-WebUI/pretrained\"\n",
        "%cd \"/content/MedleyVox-Inference-WebUI/pretrained\"\n",
        "!wget https://dl.fbaipublicfiles.com/fairseq/wav2vec/xlsr_53_56k.pt\n",
        "%cd /content/MedleyVox-Inference-WebUI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-65aiMrFDU0"
      },
      "source": [
        "# Inference\n",
        "\n",
        "### Place the audio to be processed in the 'inputs' folder, and the processing results will be stored in the 'results' folder. There are two ways to tun inference: use WebUI or use command line.\n",
        "\n",
        "- Use WebUI: Run the WebUI startup code block and then access the WebUI through the public link.\n",
        "- Use command line: Select appropriate inference parameters and run the the command line code block.\n",
        "\n",
        "### Explanation of reasoning parameters. For more infrmation, refer to `inference.py`.\n",
        "\n",
        "- `Model name`: Select which model you want to use.\n",
        "- `Use overlapadd`: Use overlapadd functions, ola, ola_norm, w2v will work with ola_window_len, ola_hop_len argugments. w2v_chunk and sf_chunk is chunk-wise processing based on VAD, so you have to specify the vad_method args. If you use sf_chunk (spectral_featrues_chunk), you also need to specify spectral_features.\n",
        "- `Separate storage`: Save results in separate folders with the same name as the input file.\n",
        "- `Output format`: Select the output format of the results.\n",
        "- `VAD method`: What method do you want to use for 'voice activity detection (vad) -- split chunks -- processing. Only valid when 'w2v_chunk' or 'sf_chunk' for args.use_overlapadd.\n",
        "- `Spectral features`: What spectral feature do you want to use in correlation calc in speaker assignment (only valid when using sf_chunk)\n",
        "- `OLA window length`: OLA window size in [sec], only valid when using ola or ola_norm. Set 0 to use the default value (None).\n",
        "- `OLA hop length`: OLA hop size in [sec], only valid when using ola or ola_norm. Set 0 to use the default value (None).\n",
        "- `Wav2Vec nth layer output`: Wav2Vec nth layer output, only valid when using w2v or w2v_chunk. For example: 0 1 2 3, default: 0\n",
        "- `Use EMA model`: Use EMA model or online model? Only vaind when args.ema it True (model trained with EMA).\n",
        "- `Mix consistent output`: Only valid when the model is trained with mixture_consistency loss.\n",
        "- `Reorder chunks`: OLA reorder chunks. Only valid when using ola or ola_norm.\n",
        "- `Skip error files`: Skip error files while separating instead of stopping.\n",
        "\n",
        "If the input is too long, it may be impossible to inference due to lack of VRAM. In that case, use `use_overlapadd`. Among the `use_overlapadd` options, \"ola\", \"ola_norm\", and \"w2v\" all work well. Use w2v_chunk or sf_chunk if these fail or as desired. You can also try experimenting with `vad_method` options spec and webrtc when using either of the \"_chunk\" methods. Chunking has proven to be very useful therefore it is on by default."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zGqIotukFDU1",
        "outputId": "dc6d00c9-9d00-45f0-87cf-68359bfcf334",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MedleyVox-Inference-WebUI\n",
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* Running on public URL: https://33272599fccacc08ca.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
            "Inference started... Click 'Forced stop inference' to stop the process.\n",
            "Inference process started, PID: 5431.\n",
            "/usr/bin/python3 inference.py --target \"vocals\" --exp_name \"Untitled Folder\" --model_dir \"checkpoints\" --use_gpu y --use_overlapadd ola --vad_method spec --spectral_features mfcc --w2v_ckpt_dir pretrained --w2v_nth_layer_output 0 --use_ema_model y --mix_consistent_out y --reorder_chunks y --skip_error y --separate_storage y --output_format wav --inference_data_dir \"temp\" --results_save_dir \"results/\"\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/MedleyVox-Inference-WebUI/inference.py\", line 15, in <module>\n",
            "    from functions import load_ola_func_with_args\n",
            "  File \"/content/MedleyVox-Inference-WebUI/functions/__init__.py\", line 2, in <module>\n",
            "    from functions.overlapadd_w2v import LambdaOverlapAdd_Wav2Vec\n",
            "  File \"/content/MedleyVox-Inference-WebUI/functions/overlapadd_w2v.py\", line 6, in <module>\n",
            "    from functions.wav2vec_feature_extractor import Wav2Vec2FeatureExtractor\n",
            "  File \"/content/MedleyVox-Inference-WebUI/functions/wav2vec_feature_extractor.py\", line 6, in <module>\n",
            "    import fairseq\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fairseq/__init__.py\", line 20, in <module>\n",
            "    from fairseq.distributed import utils as distributed_utils\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fairseq/distributed/__init__.py\", line 7, in <module>\n",
            "    from .fully_sharded_data_parallel import (\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fairseq/distributed/fully_sharded_data_parallel.py\", line 10, in <module>\n",
            "    from fairseq.dataclass.configs import DistributedTrainingConfig\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fairseq/dataclass/__init__.py\", line 6, in <module>\n",
            "    from .configs import FairseqDataclass\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fairseq/dataclass/configs.py\", line 1104, in <module>\n",
            "    @dataclass\n",
            "     ^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/dataclasses.py\", line 1232, in dataclass\n",
            "    return wrap(cls)\n",
            "           ^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/dataclasses.py\", line 1222, in wrap\n",
            "    return _process_class(cls, init, repr, eq, order, unsafe_hash,\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/dataclasses.py\", line 958, in _process_class\n",
            "    cls_fields.append(_get_field(cls, name, type, kw_only))\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/dataclasses.py\", line 815, in _get_field\n",
            "    raise ValueError(f'mutable default {type(f.default)} for field '\n",
            "ValueError: mutable default <class 'fairseq.dataclass.configs.CommonConfig'> for field common is not allowed: use default_factory\n",
            "Inference started... Click 'Forced stop inference' to stop the process.\n",
            "Inference process started, PID: 5796.\n",
            "/usr/bin/python3 inference.py --target \"vocals\" --exp_name \"Untitled Folder\" --model_dir \"checkpoints\" --use_gpu y --use_overlapadd ola --vad_method spec --spectral_features mfcc --w2v_ckpt_dir pretrained --w2v_nth_layer_output 0 --use_ema_model y --mix_consistent_out y --reorder_chunks y --skip_error y --separate_storage y --output_format wav --inference_data_dir \"temp\" --results_save_dir \"results/\"\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/MedleyVox-Inference-WebUI/inference.py\", line 15, in <module>\n",
            "    from functions import load_ola_func_with_args\n",
            "  File \"/content/MedleyVox-Inference-WebUI/functions/__init__.py\", line 2, in <module>\n",
            "    from functions.overlapadd_w2v import LambdaOverlapAdd_Wav2Vec\n",
            "  File \"/content/MedleyVox-Inference-WebUI/functions/overlapadd_w2v.py\", line 6, in <module>\n",
            "    from functions.wav2vec_feature_extractor import Wav2Vec2FeatureExtractor\n",
            "  File \"/content/MedleyVox-Inference-WebUI/functions/wav2vec_feature_extractor.py\", line 6, in <module>\n",
            "    import fairseq\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fairseq/__init__.py\", line 20, in <module>\n",
            "    from fairseq.distributed import utils as distributed_utils\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fairseq/distributed/__init__.py\", line 7, in <module>\n",
            "    from .fully_sharded_data_parallel import (\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fairseq/distributed/fully_sharded_data_parallel.py\", line 10, in <module>\n",
            "    from fairseq.dataclass.configs import DistributedTrainingConfig\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fairseq/dataclass/__init__.py\", line 6, in <module>\n",
            "    from .configs import FairseqDataclass\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/fairseq/dataclass/configs.py\", line 1104, in <module>\n",
            "    @dataclass\n",
            "     ^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/dataclasses.py\", line 1232, in dataclass\n",
            "    return wrap(cls)\n",
            "           ^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/dataclasses.py\", line 1222, in wrap\n",
            "    return _process_class(cls, init, repr, eq, order, unsafe_hash,\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/dataclasses.py\", line 958, in _process_class\n",
            "    cls_fields.append(_get_field(cls, name, type, kw_only))\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/dataclasses.py\", line 815, in _get_field\n",
            "    raise ValueError(f'mutable default {type(f.default)} for field '\n",
            "ValueError: mutable default <class 'fairseq.dataclass.configs.CommonConfig'> for field common is not allowed: use default_factory\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 3019, in block_thread\n",
            "    time.sleep(0.1)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/MedleyVox-Inference-WebUI/webui.py\", line 216, in <module>\n",
            "    webui().queue().launch(inbrowser=True, server_name=args.ip_address, server_port=args.port, share=args.share)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2925, in launch\n",
            "    self.block_thread()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 3023, in block_thread\n",
            "    self.server.close()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/http_server.py\", line 69, in close\n",
            "    self.thread.join(timeout=5)\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1123, in join\n",
            "    self._wait_for_tstate_lock(timeout=max(timeout, 0))\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1139, in _wait_for_tstate_lock\n",
            "    if lock.acquire(block, timeout):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "Killing tunnel 127.0.0.1:7860 <> https://33272599fccacc08ca.gradio.live\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "#@title Run inference in WebUI\n",
        "#@markdown # Run inference in WebUI\n",
        "#@markdown\n",
        "\n",
        "#@markdown\n",
        "\n",
        "#@markdown Language Setting\n",
        "language = \"English\" #@param [\"English\", \"简体中文\"]\n",
        "\n",
        "import os\n",
        "language_dict = {\"English\": \"en_US\", \"简体中文\": \"zh_CN\"}\n",
        "os.environ[\"LANGUAGE\"] = language_dict[language]\n",
        "\n",
        "%cd /content/MedleyVox-Inference-WebUI\n",
        "!python webui.py -s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8N73K6cFDU3"
      },
      "outputs": [],
      "source": [
        "#@title Run inference in Command Line\n",
        "#@markdown # Run inference in Command Line\n",
        "#@markdown\n",
        "\n",
        "#@markdown\n",
        "\n",
        "#@markdown File and model Parameters\n",
        "folder_input = \"inputs\" #@param {type:\"string\"}\n",
        "store_dir = \"results\" #@param {type:\"string\"}\n",
        "model_name = \"vocals 238\" #@param [\"vocals_238\", \"multi_singing_librispeech_138\", \"singing_librispeech_ft_iSRNet\"]\n",
        "\n",
        "#@markdown\n",
        "\n",
        "#@markdown Common Parameters\n",
        "use_overlapadd = \"ola\" #@param [\"None\", \"ola\", \"ola_norm\", \"w2v\", \"w2v_chunk\", \"sf_chunk\"]\n",
        "separate_storage = False #@param {type:\"boolean\"}\n",
        "skip_error = True #@param {type:\"boolean\"}\n",
        "output_format = \"wav\" #@param [\"wav\", \"flac\", \"mp3\"]\n",
        "\n",
        "#@markdown\n",
        "\n",
        "#@markdown Advanced Parameters\n",
        "vad_method = \"spec\" #@param [\"spec\", \"webrtc\"]\n",
        "spectral_features = \"mfcc\" #@param [\"mfcc\", \"spectral_centroid\"]\n",
        "ola_window_len = \"0\" #@param {type:\"string\"}\n",
        "ola_hop_len = \"0\" #@param {type:\"string\"}\n",
        "w2v_nth_layer_output = \"0\" #@param {type:\"string\"}\n",
        "use_ema_model = True #@param {type:\"boolean\"}\n",
        "mix_consistent_out = True #@param {type:\"boolean\"}\n",
        "reorder_chunks = True #@param {type:\"boolean\"}\n",
        "\n",
        "import os\n",
        "import glob\n",
        "\n",
        "MODEL_DIR = \"checkpoint\"\n",
        "PRETRAINED_MODEL_DIR = \"pretrained\"\n",
        "use_gpu = True\n",
        "\n",
        "model_file = os.path.basename(glob.glob(os.path.join(MODEL_DIR, model_name, \"*.pth\"))[0])\n",
        "target = model_file.replace(\".pth\", \"\")\n",
        "exp_name = model_name\n",
        "model_dir = MODEL_DIR\n",
        "params = f\"--target \\\"{target}\\\" --exp_name \\\"{exp_name}\\\" --model_dir \\\"{model_dir}\\\"\"\n",
        "if use_gpu:\n",
        "    params += \" --use_gpu y\"\n",
        "else:\n",
        "    params += \" --use_gpu n\"\n",
        "if use_overlapadd != \"None\":\n",
        "    params += f\" --use_overlapadd {use_overlapadd}\"\n",
        "params += f\" --vad_method {vad_method} --spectral_features {spectral_features} --w2v_ckpt_dir {PRETRAINED_MODEL_DIR} --w2v_nth_layer_output {w2v_nth_layer_output}\"\n",
        "if ola_window_len != \"0\":\n",
        "    params += f\" --ola_window_len {ola_window_len}\"\n",
        "if ola_hop_len != \"0\":\n",
        "    params += f\" --ola_hop_len {ola_hop_len}\"\n",
        "if use_ema_model:\n",
        "    params += \" --use_ema_model y\"\n",
        "else:\n",
        "    params += \" --use_ema_model n\"\n",
        "if mix_consistent_out:\n",
        "    params += \" --mix_consistent_out y\"\n",
        "else:\n",
        "    params += \" --mix_consistent_out n\"\n",
        "if reorder_chunks:\n",
        "    params += \" --reorder_chunks y\"\n",
        "else:\n",
        "    params += \" --reorder_chunks n\"\n",
        "if skip_error:\n",
        "    params += \" --skip_error y\"\n",
        "else:\n",
        "    params += \" --skip_error n\"\n",
        "if separate_storage:\n",
        "    params += f\" --separate_storage y\"\n",
        "else:\n",
        "    params += f\" --separate_storage n\"\n",
        "params += f\" --output_format {output_format} --inference_data_dir \\\"{folder_input}\\\" --results_save_dir \\\"{store_dir}\\\"\"\n",
        "print(params)\n",
        "\n",
        "%cd /content/MedleyVox-Inference-WebUI\n",
        "!python inference.py {params}"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}